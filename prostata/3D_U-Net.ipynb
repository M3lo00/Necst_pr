{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. Load data\n",
    "    * 1.1. Helper functions for loading the data\n",
    "    * 1.2. Generate dataset and export to .pkl files\n",
    "    * 1.3. Visualise scans and annotations\n",
    "    * 1.4. Class frequencies and weights\n",
    "2. Cross-validation dataset [extension]\n",
    "3. Data augmentation\n",
    "    * 3.1. Helper methods for data augmentation\n",
    "    * 3.2. Visualise data augmentations\n",
    "4. Data pre-processing\n",
    "    * 4.1. Histogram equalisation\n",
    "        * 4.1.1. Helper methods for histogram equalisation\n",
    "        * 4.1.2. Apply histogram equalisation to dataset\n",
    "        * 4.1.3. Visualise histogram equalisation\n",
    "    * 4.2. Input/output scaling and IOU evaluation\n",
    "        * 4.2.1. Helper functions for input/output scaling and IOU evaluation\n",
    "        * 4.2.2. Visualise scaled scans and annotations\n",
    "5. 3D U-Net model\n",
    "6. Model training\n",
    "    * 6.1. Generate pre-processed augmented dataset\n",
    "    * 6.2. Set model hyperparameters\n",
    "    * 6.3. Helper functions for training\n",
    "    * 6.4. Train model\n",
    "7. Model inference\n",
    "    * 7.1. Visualise predictions\n",
    "        * 7.1.1. Example: 'best' prediction\n",
    "        * 7.1.2. Example: 'worst' prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Install the relevant Python 3 dependencies specified in *requirements.txt*. \n",
    "> `pip3 install -r requirements.txt` \n",
    "2. Download and extract the dataset: __[*3D Prostate Structures*](https://wiki.cancerimagingarchive.net/display/Public/NCI-ISBI+2013+Challenge+-+Automated+Segmentation+of+Prostate+Structures)__ (or directly [here](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=21267207))\n",
    "3. To train the 3D U-Net model, execute cells in sections 1-6.\n",
    "4. To test the trained 3D U-Net model, execute cells in section 7. \n",
    "\n",
    "*Note:* For interactive plot tool, Jupyter must be run locally (won't work if viewing via Github). \n",
    "\n",
    "As a starting point, I used the implementation from https://github.com/96imranahmed/3D-Unet. \n",
    "My major changes and additions are in sections 1.4, 3.1, 3.2, 4.1, 5, 6.3, 6.4 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pydicom\n",
    "#%pip install pynrrd\n",
    "#%pip install scikit-image\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pydicom\n",
    "import nrrd\n",
    "import scipy.ndimage\n",
    "import scipy.misc\n",
    "import pickle\n",
    "import random\n",
    "import skimage\n",
    "%matplotlib notebook\n",
    "\n",
    "if sys.version_info[0] != 3:\n",
    "    raise Exception(\"Python version 3 has to be used!\")\n",
    "\n",
    "print(\"Currently using\")\n",
    "print(\"\\t numpy \", np.__version__)\n",
    "print(\"\\t scipy \", scipy.__version__)\n",
    "print(\"\\t matplotlib \", matplotlib.__version__)\n",
    "#print(\"\\t tensorflow \", tf.__version__)\n",
    "print(\"\\t pydicom \", pydicom.__version__)\n",
    "print(\"\\t nrrd \", nrrd.__version__)\n",
    "print(\"\\t skimage \", skimage.__version__)\n",
    "\n",
    "np.random.seed(37) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Helper functions for loading the data\n",
    "* read MRI scans (.dcm) and annotations (.nddr)\n",
    "* match scans with annotations \n",
    "* visualise 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dcm(file_path, check_term = 'Prostate'):\n",
    "    # Read all DCM (slices) files within a directory and order the files based on filename\n",
    "    out_dcm = {}\n",
    "    for dirName, subdirList, fileList in os.walk(file_path): #while searching all the directories starding from the one given and proceding top-down like a tree exploration. Yelds a tuple with directory name, list of sub directories, list of files\n",
    "        c_dcm = []\n",
    "        cur_name = \"\"\n",
    "        dir_split = dirName.split(\"/\")\n",
    "        for f_chk in dir_split:\n",
    "            if check_term in f_chk:\n",
    "                cur_name = f_chk\n",
    "        for filename in fileList:\n",
    "            if \".dcm\" in filename.lower():\n",
    "                name = (int)((os.path.splitext(filename)[0]).split(\"-\")[1])\n",
    "                c_dcm.append((os.path.join(dirName,filename), name))\n",
    "        if len(c_dcm) > 0:\n",
    "            c_dcm = sorted(c_dcm, key = lambda t: t[1]) # Sort into correct order\n",
    "            out_dcm[cur_name] = [c[0] for c in c_dcm]   # Store in dictionary\n",
    "    return out_dcm\n",
    "\n",
    "def return_nrrd(file_path):\n",
    "    # Read all NRRD (annotation) files within a directory\n",
    "    out_nrrd = {}\n",
    "    for dirName, subdirList, fileList in os.walk(file_path):\n",
    "        for filename in fileList:\n",
    "            if \".nrrd\" in filename.lower():\n",
    "                name = filename.split('_')[0] \n",
    "                name = name.split('.')[0] # Get annotation name and store in dictionary\n",
    "                out_nrrd[name] = os.path.join(dirName,filename)\n",
    "    return out_nrrd\n",
    "\n",
    "def get_dataset(data_dir, anns_dir):\n",
    "    # Match DCM volumes with corresponding annotation files\n",
    "    data_out = []\n",
    "    shapes = {}\n",
    "    d_dcm = return_dcm(data_dir)\n",
    "    d_nrrd = return_nrrd(anns_dir)\n",
    "    for i in d_nrrd:\n",
    "        seg, opts = nrrd.read(d_nrrd[i])\n",
    "        voxels = np.zeros(np.shape(seg))\n",
    "        for j in range(len(d_dcm[i])):\n",
    "            dicom_ref = pydicom.read_file(d_dcm[i][j])\n",
    "            found = False\n",
    "            chk_val = dicom_ref[(\"0020\", \"0013\")].value\n",
    "            # Make sure you get the right slice! This is a bizarre specification thing related to DCM dataset\n",
    "            # Note, if you just use the default filename ordering you get mismatched slices!\n",
    "            if int(chk_val.__str__()) - 1 < np.shape(voxels)[-1]:\n",
    "                voxels[:, :, int(chk_val.__str__()) - 1] = dicom_ref.pixel_array\n",
    "            else: \n",
    "                print('Index: ',str(int(chk_val.__str__()) - 1), ' too large for ', i, ' skipping!')\n",
    "        # Rotate and flip annotations to match volumes\n",
    "        seg = scipy.ndimage.interpolation.rotate(seg, 90, reshape = False)\n",
    "        for i in range(np.shape(seg)[2]):\n",
    "            cur_img = np.squeeze(seg[:, :, i])\n",
    "            seg[:, :, i] = np.flipud(cur_img)\n",
    "        # Store volume shapes (for debug)\n",
    "        if voxels.shape in shapes:\n",
    "            shapes[voxels.shape] += 1\n",
    "        else:\n",
    "            shapes[voxels.shape] = 1\n",
    "        # Saves data\n",
    "        data_out.append((voxels, seg))\n",
    "    return data_out\n",
    "\n",
    "def plot_slice(slice_in, is_anns = False, num_anns = 4):\n",
    "    # Plot a slice of data - can either be raw image data or corresponding annotation\n",
    "    slice_in = np.squeeze(slice_in)\n",
    "    plt.figure()\n",
    "    plt.set_cmap(plt.bone())\n",
    "    if is_anns:\n",
    "        plt.pcolormesh(slice_in, vmin = 0, vmax = num_anns - 1)\n",
    "    else:\n",
    "        plt.pcolormesh(slice_in)\n",
    "    plt.show()\n",
    "\n",
    "##########################################################################\n",
    "# Multi-slice view code extracted and adapted from: \n",
    "# https://www.datacamp.com/community/tutorials/matplotlib-3d-volumetric-data\n",
    "\n",
    "def multi_slice_viewer(feats, anns = None, preds = None, num_classes = 4, no_axis=False):\n",
    "    # Plot feats, anns, predictions in multi-slice-view\n",
    "    # feats OR feats + anns OR feats + anns + preds\n",
    "    if anns is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.volume = feats\n",
    "        ax.index = feats.shape[-1] // 2\n",
    "        ax.imshow(feats[:, :, ax.index],  cmap='bone')\n",
    "        fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "    else:\n",
    "        if preds is None:\n",
    "            fig, axarr = plt.subplots(1, 2)\n",
    "            plt.tight_layout()\n",
    "            axarr[0].volume = feats\n",
    "            axarr[0].index = 0\n",
    "            axarr[0].imshow(feats[:, :, axarr[0].index],  cmap='bone')\n",
    "            axarr[0].set_title('Scans')\n",
    "            axarr[1].volume = anns\n",
    "            axarr[1].index = 0\n",
    "            axarr[1].imshow(anns[:, :, axarr[1].index],  cmap='bone', vmin = 0, vmax = num_classes)\n",
    "            axarr[1].set_title('Annotations')\n",
    "            fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "        else:\n",
    "            fig, axarr = plt.subplots(1, 3)\n",
    "            plt.tight_layout()\n",
    "            axarr[0].volume = feats\n",
    "            axarr[0].index = 0\n",
    "            axarr[0].imshow(feats[:, :, axarr[0].index],  cmap='bone')\n",
    "            axarr[0].set_title('Scans')\n",
    "            axarr[1].volume = anns\n",
    "            axarr[1].index = 0\n",
    "            axarr[1].imshow(anns[:, :, axarr[1].index],  cmap='bone', vmin = 0, vmax = num_classes)\n",
    "            axarr[1].set_title('Annotations')\n",
    "            axarr[2].volume = preds\n",
    "            axarr[2].index = 0\n",
    "            axarr[2].imshow(preds[:, :, axarr[2].index],  cmap='bone', vmin = 0, vmax = num_classes)\n",
    "            axarr[2].set_title('Predictions')\n",
    "            fig.canvas.mpl_connect('key_press_event', process_key)\n",
    "        if no_axis:\n",
    "            for a in axarr:\n",
    "                a.set_axis_off()\n",
    "            \n",
    "def process_key(event):\n",
    "    # Process key_press events\n",
    "    fig = event.canvas.figure\n",
    "    if event.key == 'j':\n",
    "        for ax in fig.axes: \n",
    "            previous_slice(ax)\n",
    "    elif event.key == 'k':\n",
    "        for ax in fig.axes: \n",
    "            next_slice(ax)            \n",
    "    fig.canvas.draw()\n",
    "\n",
    "def previous_slice(ax):\n",
    "    # Go to the previous slice\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index - 1) % volume.shape[-1]  # wrap around using %\n",
    "    ax.images[0].set_array(volume[:, :, ax.index])\n",
    "\n",
    "def next_slice(ax):\n",
    "    # Go to the next slice\n",
    "    volume = ax.volume\n",
    "    ax.index = (ax.index + 1) % volume.shape[-1]\n",
    "    ax.images[0].set_array(volume[:, :, ax.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Generate dataset and export to .pkl files\n",
    "\n",
    "Generates dataset from specified folders and saves it to pickle files. <br>\n",
    "Leaderboard dataset is used as a validation set (in order to compare later design decisions and to decide when to stop training). <br>\n",
    "*Note:* This step does not need to be performed again. \n",
    "\n",
    "#### Important:\n",
    "Change the below paths to data folders, if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .dcm data files\n",
    "data_train_dir = './training/'              \n",
    "data_leader_dir = './leaderboard/' \n",
    "data_test_dir = './testing/'\n",
    "\n",
    "# .nrrd annotation files\n",
    "anns_train_dir = './Training_nrrd/'         \n",
    "anns_leader_dir = './Leaderboard_nrrd/'   \n",
    "anns_test_dir = './Test_nrrd/'          \n",
    "\n",
    "train = get_dataset(data_train_dir, anns_train_dir)\n",
    "valid = get_dataset(data_leader_dir, anns_leader_dir)\n",
    "test = get_dataset(data_test_dir, anns_test_dir)\n",
    "\n",
    "if not os.path.exists('./pickles'):\n",
    "    os.makedirs('./pickles')\n",
    "pickle.dump(file = open('./pickles/train.pkl', 'wb'), obj = train)\n",
    "pickle.dump(file = open('./pickles/valid.pkl', 'wb'), obj = valid)\n",
    "pickle.dump(file = open('./pickles/test.pkl', 'wb'), obj = test)\n",
    "\n",
    "print(\"\\nTraining scans:\", len(train), \"\\t\\t Scan slices:\", np.sum([x.shape[2] for x,_ in train]), \n",
    "      \"\\nValidation scans:\", len(valid), \"\\t\\t Scan slices:\", np.sum([x.shape[2] for x,_ in valid]), \n",
    "      \"\\nTesting scans: \", len(test), \"\\t\\t Scan slices:\", np.sum([x.shape[2] for x,_ in test]))\n",
    "print(\"Sample 3D scans' shapes:\", train[2][0].shape, valid[1][0].shape, test[4][0].shape) # as we can see these shapes vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample 3D scans' shapes:\", train[3][0].shape, valid[1][0].shape, test[4][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Visualise scans and annotations\n",
    "\n",
    "We can verify that the loaded scans and annotations match for a sample dataset example. This indicates that the features and labels are correctly formatted. \n",
    "\n",
    "Annotation labels: \n",
    "* 0: background - black \n",
    "* 1: peripheral zone (PZ) - dark gray \n",
    "* 2: central gland (CG) - bright gray \n",
    "\n",
    "**Press 'j' or 'k' to cycle through the plots in depth (Jupyter must be run locally!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_id = 12\n",
    "multi_slice_viewer(train[img_id][0], train[img_id][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Class frequencies and weights\n",
    "Class weights (inversely proportional to class frequencies) will be used in the weighted cross-entropy loss function to handle class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class_freq = {0:0, 1:0, 2:0}\n",
    "for i in range(len(train)):\n",
    "    for j in range(train[i][1].shape[2]):\n",
    "        d = Counter(train[i][1][:,:,j].flatten())\n",
    "        class_freq[0] += d[0]\n",
    "        class_freq[1] += d[1]\n",
    "        class_freq[2] += d[2]\n",
    "print(\"Class frequencies in training set: \", class_freq)\n",
    "\n",
    "inv_class_freq = 1. / np.array([class_freq[0], class_freq[1], class_freq[2]], dtype=np.float64)\n",
    "class_weights = inv_class_freq / sum(inv_class_freq)\n",
    "print(\"Class weights (inversely proportional to class frequencies): \", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cross-validation split [extension]\n",
    "\n",
    "Perform k-fold cross validation, as suggested in the 3D U-Net paper. \n",
    "For each fold, this needs to be run again. \n",
    "\n",
    "*Note:* **This is left as an extension and it was not performed as it would be very time-consuming. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cross_val = False # Whether to do cross-validation\n",
    "if do_cross_val:\n",
    "    data_total = train + valid + test\n",
    "    K_FOLD = 3\n",
    "    VALID_FRAC = 0.25 # fraction of the training set used as validation set\n",
    "    CURRENT_FOLD = 0  # need to be set to: 0, 1, ... K_FOLD-1\n",
    "\n",
    "    val_split = len(data_total)/K_FOLD\n",
    "    val_idx = CURRENT_FOLD*val_split\n",
    "    train = data_total[:val_idx] + data_total[val_idx+val_split:]\n",
    "    valid = train[:int(len(train)*VALID_FRAC)]\n",
    "    train = train[int(len(train)*VALID_FRAC):]\n",
    "    test = data_total[val_idx:val_idx+val_split]\n",
    "    data_total = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data augmentation\n",
    "\n",
    "As suggested in the paper, the training dataset is **randomly** augmented with minor: \n",
    "* rotations, \n",
    "* scales and crops,\n",
    "* gray value augmentations, \n",
    "* and elastic distortions (smooth dense deformation field). \n",
    "\n",
    "\n",
    "Data augmentation is performed in the pre-processing stage (as opposed to the augmentation on-the-fly). This implies:\n",
    "* faster training \n",
    "* higher memory requirements (this is probably the reason why the paper chose the augmentation on-the-fly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Helper methods for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(voxels, lbls, theta = None):\n",
    "    # Rotate volume by a minor angle (+/- 10 degrees: determined by investigation of dataset variability)\n",
    "    if theta is None:\n",
    "        theta = random.randint(-10, 10)\n",
    "    vox_new = scipy.ndimage.interpolation.rotate(voxels, theta, reshape = False)\n",
    "    lbl_new = scipy.ndimage.interpolation.rotate(lbls, theta, reshape = False)\n",
    "    return vox_new, lbl_new\n",
    "\n",
    "def scale_and_crop(voxels, lbls):\n",
    "    # Scale the volume by a minor size and crop around centre (can also modify for random crop)\n",
    "    o_s = voxels.shape\n",
    "    r_s = [0]*len(o_s)\n",
    "    scale_factor = random.uniform(1, 1.2)\n",
    "    vox_zoom = scipy.ndimage.interpolation.zoom(voxels, scale_factor, order=1)\n",
    "    lbl_zoom = scipy.ndimage.interpolation.zoom(lbls, scale_factor, order=0)\n",
    "    new_shape = vox_zoom.shape\n",
    "    # Start with offset\n",
    "    for i in range(len(o_s)):\n",
    "        if new_shape[i] == 1: \n",
    "            r_s[i] = 0\n",
    "            continue\n",
    "        r_c = int(((new_shape[i] - o_s[i]) - 1)/2)\n",
    "        r_s[i] = r_c\n",
    "    r_e = [r_s[i] + o_s[i] for i in list(range(len(o_s)))]\n",
    "    vox_zoom = vox_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
    "    lbl_zoom = lbl_zoom[r_s[0]:r_e[0], r_s[1]:r_e[1], r_s[2]:r_e[2]]\n",
    "    return vox_zoom, lbl_zoom\n",
    "\n",
    "def grayscale_variation(voxels, lbls):\n",
    "    # Introduce a random global increment in gray-level value of volume. \n",
    "    im_min = np.min(voxels)\n",
    "    im_max = np.max(voxels)\n",
    "    mean = np.random.normal(0, 0.1)\n",
    "    smp = np.random.normal(mean, 0.01, size = np.shape(voxels))\n",
    "    voxels = voxels + im_max*smp\n",
    "    voxels[voxels <= im_min] = im_min # Clamp to min value\n",
    "    voxels[voxels > im_max] = im_max  # Clamp to max value\n",
    "    return voxels, lbls\n",
    "\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "def elastic_deformation(voxels, lbls, alpha=None, sigma=None, mode=\"constant\", cval=0, is_random=False): \n",
    "    # Apply elastic deformation/distortion to the wolume\n",
    "    # Adapted from: https://tensorlayer.readthedocs.io/en/stable/_modules/tensorlayer/prepro.html#elastic_transform\n",
    "    if alpha == None:\n",
    "        alpha=voxels.shape[1]*3.\n",
    "    if sigma == None:\n",
    "        sigma=voxels.shape[1]*0.07\n",
    "    if is_random is False:\n",
    "        random_state = np.random.RandomState(None)\n",
    "    else:\n",
    "        random_state = np.random.RandomState(int(time.time()))\n",
    "        \n",
    "    if len(voxels.shape) == 3:\n",
    "        voxels = np.reshape(voxels, (voxels.shape[0], voxels.shape[1], voxels.shape[2], 1) )\n",
    "        lbls = np.reshape(lbls, (lbls.shape[0], lbls.shape[1], lbls.shape[2], 1) )\n",
    "        \n",
    "    shape = (voxels.shape[0], voxels.shape[1])\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=mode, cval=cval) * alpha\n",
    "    x_, y_ = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "    indices = np.reshape(x_ + dx, (-1, 1)), np.reshape(y_ + dy, (-1, 1))\n",
    "    \n",
    "    new_voxels = np.zeros(voxels.shape)\n",
    "    new_lbls = np.zeros(lbls.shape)\n",
    "    for i in range(voxels.shape[2]): # apply the same distortion to each slice within the volume\n",
    "        new_voxels[:,:,i,0] = map_coordinates(voxels[:,:,i,0], indices, order=1).reshape(shape)\n",
    "        new_lbls[:,:,i,0] = map_coordinates(lbls[:,:,i,0], indices, order=1).reshape(shape)\n",
    "        \n",
    "    return new_voxels, new_lbls\n",
    "\n",
    "def sample_with_p(p):\n",
    "    # Helper function to return boolean of a sample with given probability p\n",
    "    if random.random() < p:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_random_perturbation(voxels, lbls):\n",
    "    # Generate a random perturbation of the input feature + label\n",
    "    p_rotate = 0.6\n",
    "    p_scale = 0.6\n",
    "    p_gray = 0.6\n",
    "    p_deform = 0.6\n",
    "    new_voxels, new_lbls = voxels, lbls\n",
    "    if sample_with_p(p_rotate):\n",
    "        new_voxels, new_lbls = rotate(new_voxels, new_lbls)\n",
    "    if sample_with_p(p_scale):\n",
    "        new_voxels, new_lbls = scale_and_crop(new_voxels, new_lbls)\n",
    "    if sample_with_p(p_gray):\n",
    "        new_voxels, new_lbls = grayscale_variation(new_voxels, new_lbls)\n",
    "    if sample_with_p(p_deform):\n",
    "        new_voxels, new_lbls = elastic_deformation(new_voxels, new_lbls)\n",
    "    return new_voxels, new_lbls\n",
    "\n",
    "def plot_augmentation(img_org, ann_org, img_aug, ann_aug, title_aug='Augmented', axis_off=True):\n",
    "    # Plot original and augmented image along with its annotation\n",
    "    # Works for different kinds of augmentations\n",
    "    if len(ann_org) == 0:\n",
    "        n = 1 # Annotations are not plotted\n",
    "    else:\n",
    "        n = 2 # Annotations are plotted\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.subplot(n,2,1)\n",
    "    if axis_off:\n",
    "        plt.axis('off')\n",
    "    plt.title(\"Original scan\")\n",
    "    plt.imshow(img_org, cmap=plt.cm.gray)\n",
    "    plt.subplot(n,2,2)\n",
    "    if axis_off:\n",
    "        plt.axis('off')\n",
    "    plt.title(title_aug)\n",
    "    plt.imshow(img_aug, cmap=plt.cm.gray)\n",
    "    if n > 1:\n",
    "        plt.subplot(n,2,3)\n",
    "        if axis_off:\n",
    "            plt.axis('off')\n",
    "        plt.title(\"Original annotation\")\n",
    "        plt.imshow(ann_org, cmap=plt.cm.gray)\n",
    "        plt.subplot(n,2,4)\n",
    "        if axis_off:\n",
    "            plt.axis('off')\n",
    "        plt.title(title_aug)\n",
    "        plt.imshow(ann_aug, cmap=plt.cm.gray)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Visualise data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset if needed\n",
    "# train = pickle.load(file = open('./pickles/train.pkl', 'rb'))\n",
    "\n",
    "img_id = 12\n",
    "slice_id = 8\n",
    "\n",
    "imgs_org = train[img_id][0]\n",
    "anns_org = train[img_id][1]\n",
    "\n",
    "img_org = train[img_id][0][:,:,slice_id]\n",
    "ann_org = train[img_id][1][:,:,slice_id]\n",
    "\n",
    "# Rotation\n",
    "imgs_aug, anns_aug = rotate(imgs_org, anns_org, theta=10)\n",
    "plot_augmentation(img_org, ann_org, imgs_aug[:,:,slice_id], anns_aug[:,:,slice_id], title_aug='Rotation')\n",
    "\n",
    "# Scaling\n",
    "imgs_aug, anns_aug = scale_and_crop(imgs_org, anns_org)\n",
    "plot_augmentation(img_org, ann_org, imgs_aug[:,:,slice_id], anns_aug[:,:,slice_id], title_aug='Scaling')\n",
    "\n",
    "# Gray value variation\n",
    "imgs_aug, anns_aug = grayscale_variation(imgs_org, anns_org)\n",
    "plot_augmentation(img_org, [], imgs_aug[:,:,slice_id], [], title_aug='Gray variation')\n",
    "\n",
    "# Elastic deformation (smooth dense deformation field)\n",
    "imgs_aug, anns_aug = elastic_deformation(imgs_org, anns_org)\n",
    "plot_augmentation(img_org, ann_org, imgs_aug[:,:,slice_id,0], anns_aug[:,:,slice_id,0], title_aug='Elastic deformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Histogram equalisation\n",
    "\n",
    "Histogram equalisation (HE) is a technique used to enhance low-contrast images, by spreading out the most frequent intenshity values. The resulting equalised image has a roughly linear cumulative distribution function. \n",
    "\n",
    "HE is applied to all three paritions: train, valid and test. It further ensures that the image intensity values are within the $\\left[0,1\\right]$ range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Helper methods for histogram equalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure, img_as_float\n",
    "\n",
    "def hist_equalise(dataset):\n",
    "    # Perform histogram equalisation on each scan slice \n",
    "    for i in range(len(dataset)): # Over subjects\n",
    "        for j in range(dataset[i][0].shape[2]): # Over scan slices\n",
    "            dataset[i][0][:,:,j] = exposure.equalize_hist(dataset[i][0][:,:,j])\n",
    "            \n",
    "def plot_img_and_hist(image, axes, bins=100):\n",
    "    # Plot an image along with its histogram and cumulative histogram.\n",
    "    # Code adapted from: http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html\n",
    "    image = img_as_float(image)\n",
    "    ax_img, ax_hist = axes\n",
    "    ax_cdf = ax_hist.twinx()\n",
    "    \n",
    "    # Display image\n",
    "    ax_img.imshow(image, cmap=plt.cm.gray)\n",
    "    ax_img.set_axis_off()\n",
    "    \n",
    "    # Display histogram\n",
    "    ax_hist.hist(image.ravel(), bins=bins, histtype='step', color='black')\n",
    "    ax_hist.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n",
    "    ax_hist.set_xlabel('Pixel intensity')\n",
    "    ax_hist.set_xlim(0, 1)\n",
    "    ax_hist.set_yticks([])\n",
    "\n",
    "    # Display cumulative distribution\n",
    "    img_cdf, bins = exposure.cumulative_distribution(image, bins)\n",
    "    ax_cdf.plot(bins, img_cdf, 'r')\n",
    "    ax_cdf.set_yticks([])\n",
    "\n",
    "    return ax_img, ax_hist, ax_cdf\n",
    "\n",
    "def plot_histogram_equalisation(img_org, img_heq):\n",
    "    # Plot original low contrast image and histogram eualised image, with their histograms and cumulative histograms.\n",
    "    # Code adapted from: http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    axes = np.zeros((2, 2), dtype=np.object)\n",
    "    axes[0, 0] = fig.add_subplot(2, 2, 1)\n",
    "    for i in range(1, 2):\n",
    "        axes[0, i] = fig.add_subplot(2, 2, 1+i, sharex=axes[0,0], sharey=axes[0,0])\n",
    "    for i in range(0, 2):\n",
    "        axes[1, i] = fig.add_subplot(2, 2, 3+i)\n",
    "\n",
    "    ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_org, axes[:, 0])\n",
    "    ax_img.set_title('Original low contrast image')\n",
    "\n",
    "    y_min, y_max = ax_hist.get_ylim()\n",
    "    ax_hist.set_ylabel('Number of pixels')\n",
    "    ax_hist.set_yticks(np.linspace(0, y_max, 5))\n",
    "\n",
    "    ax_img, ax_hist, ax_cdf = plot_img_and_hist(img_heq, axes[:, 1])\n",
    "    ax_img.set_title('Histogram equalised image')\n",
    "\n",
    "    ax_cdf.set_ylabel('Fraction of total intensity', color='red')\n",
    "    ax_cdf.set_yticks(np.linspace(0, 1, 5))\n",
    "\n",
    "    fig.tight_layout(h_pad=-1.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Apply histogram equalisation to dataset\n",
    "\n",
    "*Note:* This step does not need to be performed again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the non-histogram equalised dataset if needed\n",
    "train = pickle.load(file = open('./pickles/train.pkl', 'rb'))\n",
    "valid = pickle.load(file = open('./pickles/valid.pkl', 'rb'))\n",
    "test = pickle.load(file = open('./pickles/test.pkl', 'rb'))\n",
    "\n",
    "hist_equalise(train) # in-place\n",
    "hist_equalise(valid)\n",
    "hist_equalise(test)\n",
    "\n",
    "# Save histogram equalised dataset\n",
    "pickle.dump(file = open('./pickles/heq_train.pkl', 'wb'), obj = train)\n",
    "pickle.dump(file = open('./pickles/heq_valid.pkl', 'wb'), obj = valid)\n",
    "pickle.dump(file = open('./pickles/heq_test.pkl', 'wb'), obj = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Visualise histogram equalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the non-histogram equalised dataset as needed\n",
    "# nhe_train = pickle.load(file = open('./pickles/train.pkl', 'rb'))\n",
    "# nhe_valid = pickle.load(file = open('./pickles/valid.pkl', 'rb'))\n",
    "nhe_test = pickle.load(file = open('./pickles/test.pkl', 'rb'))\n",
    "# Load the histogram equalised dataset as needed\n",
    "# train = pickle.load(file = open('./pickles/heq_train.pkl', 'rb'))\n",
    "# valid = pickle.load(file = open('./pickles/heq_valid.pkl', 'rb'))\n",
    "test = pickle.load(file = open('./pickles/heq_test.pkl', 'rb'))\n",
    "\n",
    "# Show HE on a sample image\n",
    "img_id = 4\n",
    "slice_id = 5\n",
    "img = nhe_test[img_id][0][:,:,slice_id]\n",
    "img = img / np.max(img) # Scale to 0-1 range\n",
    "img_eq = test[img_id][0][:,:,slice_id] # Already scaled to 0-1 range\n",
    "plot_histogram_equalisation(img, img_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2. Input/output scaling and IOU evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1. Helper functions for input/output scaling and IOU evaluation\n",
    "* input/output scaling: generate volume and label tensors that can be passed into the Tensorflow format and in the default `channel_last` ordering expected by Tensorflow\n",
    "* calculate Mean IOU (with respect to original unscaled labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import os\n",
    "import sys\n",
    "import pydicom\n",
    "import nrrd\n",
    "import scipy.ndimage\n",
    "import random\n",
    "import pickle\n",
    "%matplotlib notebook\n",
    "\n",
    "INPUT_SIZE = 120 # Input feature width/height\n",
    "OUTPUT_SIZE = 120 # Output feature width/height (as defined by model)\n",
    "INPUT_DEPTH = 12 # Input depth \n",
    "OFF_IMAGE_FILL = 0 # What to fill an image with if padding is required to make Tensor\n",
    "OFF_LABEL_FILL = 0 # What to fill a label with if padding is required to make Tensor\n",
    "OUTPUT_CLASSES = 3 # Number of output classes in dataset: the fourth class can be used for unlabelled datapoints (not needed for this dataset)\n",
    "\n",
    "# Get 'natural' OUTPUT_DEPTH according to scipy method\n",
    "io_zoom = OUTPUT_SIZE/INPUT_SIZE\n",
    "zero_chk = np.zeros((INPUT_SIZE, INPUT_SIZE, INPUT_DEPTH))\n",
    "OUTPUT_DEPTH = np.shape(scipy.ndimage.interpolation.zoom(zero_chk, io_zoom, order = 1))[-1]\n",
    "# Alternatively, this can be forced to match expected tensorflow output (note, extra padding is applied if depth mismatch)\n",
    "OUTPUT_DEPTH = 12 \n",
    "\n",
    "def get_scaled_input(data, min_i = INPUT_SIZE, min_o = OUTPUT_SIZE, depth = INPUT_DEPTH, \n",
    "                    depth_out = OUTPUT_DEPTH, image_fill = OFF_IMAGE_FILL, \n",
    "                    label_fill = OFF_LABEL_FILL, n_classes = OUTPUT_CLASSES, norm_max = 500):\n",
    "    \n",
    "    # Takes raw data (x, y) and scales to match desired input and output sizes to feed into Tensorflow\n",
    "    # Pads and normalises input and also moves axes around to orientation expected by tensorflow\n",
    "    \n",
    "    input_scale_factor = min_i/data[0].shape[0]\n",
    "    output_scale_factor = min_o/data[0].shape[0]\n",
    "\n",
    "    vox_zoom = None\n",
    "    lbl_zoom = None\n",
    "\n",
    "    if not input_scale_factor == 1:\n",
    "        vox_zoom = scipy.ndimage.interpolation.zoom(data[0], input_scale_factor, order = 1) \n",
    "        # Order 1 is bilinear - fast and good enough\n",
    "    else:\n",
    "        vox_zoom = data[0]\n",
    "\n",
    "    if not output_scale_factor == 1:\n",
    "        lbl_zoom = scipy.ndimage.interpolation.zoom(data[1], output_scale_factor, order = 0) \n",
    "        # Order 0 is nearest neighbours: VERY IMPORTANT as it ensures labels are scaled properly (and stay discrete)\n",
    "    else:\n",
    "        lbl_zoom = data[1]   \n",
    "\n",
    "    lbl_pad = label_fill*np.ones((min_o, min_o, depth_out - lbl_zoom.shape[-1]))\n",
    "    lbl_zoom = np.concatenate((lbl_zoom, lbl_pad), 2)\n",
    "    lbl_zoom = lbl_zoom[np.newaxis, :, :, :]\n",
    "    \n",
    "    vox_pad = image_fill*np.ones((min_i, min_i, depth - vox_zoom.shape[-1]))\n",
    "    vox_zoom = np.concatenate((vox_zoom, vox_pad), 2)\n",
    "    \n",
    "    max_val = np.max(vox_zoom)\n",
    "    if not np.max(vox_zoom) == 0:\n",
    "        vox_zoom = vox_zoom * norm_max/np.max(vox_zoom)\n",
    "        \n",
    "    vox_zoom = vox_zoom[np.newaxis, :, :, :]\n",
    "\n",
    "    vox_zoom = np.swapaxes(vox_zoom, 0, -1)\n",
    "    lbl_zoom = np.swapaxes(lbl_zoom, 0, -1)\n",
    "    # Swap axes\n",
    "        \n",
    "    return vox_zoom, lbl_zoom\n",
    "\n",
    "def upscale_segmentation(lbl, shape_desired):\n",
    "    # Returns scaled up label for a given input label and desired shape. Required for Mean IOU calculation\n",
    "    \n",
    "    scale_factor = shape_desired[0]/lbl.shape[0]\n",
    "    lbl_upscale = scipy.ndimage.interpolation.zoom(lbl, scale_factor, order = 0)\n",
    "    # Order 0 EVEN more important here\n",
    "    lbl_upscale = lbl_upscale[:, :, :shape_desired[-1]]\n",
    "    if lbl_upscale.shape[-1] < shape_desired[-1]:\n",
    "        pad_zero = OFF_LABEL_FILL*np.zeros((shape_desired[0], shape_desired[1], shape_desired[2] - lbl_upscale.shape[-1]))\n",
    "        lbl_upscale = np.concatenate((lbl_upscale, pad_zero), axis = -1)\n",
    "    return lbl_upscale\n",
    "\n",
    "def get_label_accuracy(pred, lbl_original):\n",
    "    # Get pixel-wise labelling accuracy (DEMO metric)\n",
    "    \n",
    "    # Swap axes back\n",
    "    pred = swap_axes(pred)\n",
    "    pred_upscale = upscale_segmentation(pred, np.shape(lbl_original))\n",
    "    return 100*np.sum(np.equal(pred_upscale, lbl_original))/np.prod(lbl_original.shape)\n",
    "\n",
    "def get_mean_iou(pred, lbl_original, num_classes = OUTPUT_CLASSES, ret_full = False, reswap = False):\n",
    "    # Get mean IOU between input predictions and target labels. Note, method implicitly resizes as needed\n",
    "    # Ret_full - returns the full iou across all classes\n",
    "    # Reswap - if lbl_original is in tensorflow format, swap it back into the format expected by plotting tools (+ format of raw data)\n",
    "    \n",
    "    # Swap axes back \n",
    "    pred = swap_axes(pred)\n",
    "    if reswap:\n",
    "        lbl_original = swap_axes(lbl_original)\n",
    "    pred_upscale = upscale_segmentation(pred, np.shape(lbl_original))\n",
    "    iou = [1]*num_classes\n",
    "    for i in range(num_classes): \n",
    "        test_shape = np.zeros(np.shape(lbl_original))\n",
    "        test_shape[pred_upscale == i] = 1\n",
    "        test_shape[lbl_original == i] = 1\n",
    "        full_sum = int(np.sum(test_shape))\n",
    "        test_shape = -1*np.ones(np.shape(lbl_original))\n",
    "        test_shape[lbl_original == i] = pred_upscale[lbl_original == i]\n",
    "        t_p = int(np.sum(test_shape == i))\n",
    "        if not full_sum == 0:\n",
    "            iou[i] = t_p/full_sum\n",
    "    if ret_full:\n",
    "        return iou\n",
    "    else: \n",
    "        return np.mean(iou)\n",
    "    \n",
    "def swap_axes(pred):\n",
    "    # Swap those axes\n",
    "    pred = np.swapaxes(pred, -1, 0)\n",
    "    pred = np.squeeze(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2. Visualise scaled scans and annotations\n",
    "\n",
    "*Note:* Run section 1.1 if it has not been executed yet. \n",
    "\n",
    "**Press 'j' or 'k' to cycle through the plots in depth (Jupyter must be run locally!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_id = 15 # Image ID to view\n",
    "dataset = train\n",
    "x, y = get_scaled_input(dataset[img_id]) # Shows that this works - can check x,y shapes if needed\n",
    "x_swap = swap_axes(x)\n",
    "y_upscale = upscale_segmentation(swap_axes(y), np.shape(swap_axes(x)))\n",
    "multi_slice_viewer(x_swap, y_upscale)  # View scaled images and labels together \n",
    "\n",
    "# Compute mean iou with itself & upsampled data\n",
    "x, y = get_scaled_input(dataset[img_id])\n",
    "print('Mean IOU with itself')\n",
    "print(get_mean_iou(y, y, ret_full = True, reswap = True))\n",
    "print('Mean IOU with original labels')\n",
    "print(get_mean_iou(y, dataset[img_id][1], ret_full = True, reswap = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 3D U-Net model\n",
    "\n",
    "Set `simpleUNet` to choose if the Simplified 3D U-Net model (with one horizontal level removed) is used. This was inspired by https://github.com/mirzaevinom/prostate_segmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simpleUNet = False\n",
    "\n",
    "class UNetwork():\n",
    "    \n",
    "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
    "        # Produces the conv_batch_relu combination as in the paper\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "    \n",
    "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
    "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
    "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
    "        conv = tf.nn.relu(conv) \n",
    "        return conv\n",
    "\n",
    "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
    "        # Upconvolution - two different implementations: the first is as suggested in the original Unet paper and the second is a more recent version\n",
    "        # Needs to be determined if these do the same thing\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "        # upsample_routine = tf.keras.layers.UpSampling3D(size = (scale,scale,scale)) # Uses tf.resize_images\n",
    "        # tensor = upsample_routine(tensor)\n",
    "        # conv = tf.layers.conv3d(tensor, filters, kernel, stride, padding = 'same',\n",
    "        #                                 kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
    "        # use_bias = False is a tensorflow bug\n",
    "        conv = tf.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias=False, \n",
    "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
    "        return conv\n",
    "\n",
    "    def centre_crop_and_concat(self, prev_conv, up_conv):\n",
    "        # If concatenating two different sized Tensors, centre crop the first Tensor to the right size and concat\n",
    "        # Needed if you don't have padding\n",
    "        p_c_s = prev_conv.get_shape()\n",
    "        u_c_s = up_conv.get_shape()\n",
    "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
    "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
    "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
    "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
    "        up_concat = tf.concat((prev_conv_crop, up_conv), 4)\n",
    "        return up_concat\n",
    "        \n",
    "    def __init__(self, base_filt = 8, in_depth = INPUT_DEPTH, out_depth = OUTPUT_DEPTH,\n",
    "                 in_size = INPUT_SIZE, out_size = OUTPUT_SIZE, num_classes = OUTPUT_CLASSES,\n",
    "                 learning_rate = 0.001, print_shapes = True, drop = 0.2, should_pad = False):\n",
    "        # Initialise your model with the parameters defined above\n",
    "        # Print-shape is a debug shape printer for convenience\n",
    "        # Should_pad controls whether the model has padding or not\n",
    "        # Base_filt controls the number of base conv filters the model has. Note deeper analysis paths have filters that are scaled by this value\n",
    "        # Drop specifies the proportion of dropped activations\n",
    "        \n",
    "        self.base_init = tf.truncated_normal_initializer(stddev=0.1) # Initialise weights\n",
    "        self.reg_init = tf.contrib.layers.l2_regularizer(scale=0.1) # Initialise regularisation (was useful)\n",
    "        \n",
    "        self.should_pad = should_pad # To pad or not to pad, that is the question\n",
    "        self.drop = drop # Set dropout rate\n",
    "        \n",
    "        with tf.variable_scope('3DuNet'):\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "            self.do_print = print_shapes\n",
    "            self.model_input = tf.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
    "            # Define placeholders for feed_dict\n",
    "            self.model_labels = tf.placeholder(tf.int32, shape = (None, out_depth, out_size, out_size, 1))\n",
    "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('Input features shape', self.model_input.get_shape())\n",
    "                print('Labels shape', labels_one_hot.get_shape())\n",
    "                \n",
    "            # Level zero\n",
    "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
    "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
    "            # Level one\n",
    "            max_1_1 = tf.layers.max_pooling3d(conv_0_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
    "            conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
    "            conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_2 = tf.layers.dropout(conv_1_2, rate = self.drop, training = self.training)\n",
    "            # Level two\n",
    "            max_2_1 = tf.layers.max_pooling3d(conv_1_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
    "            conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
    "            conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
    "            conv_2_2 = tf.layers.dropout(conv_2_2, rate = self.drop, training = self.training)\n",
    "            \n",
    "            if simpleUNet:\n",
    "                # Level one\n",
    "                up_conv_2_1 = self.upconvolve(conv_2_2, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
    "            else:\n",
    "                # Level three\n",
    "                max_3_1 = tf.layers.max_pooling3d(conv_2_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
    "                conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
    "                conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
    "                conv_3_2 = tf.layers.dropout(conv_3_2, rate = self.drop, training = self.training)\n",
    "                # Level two\n",
    "                up_conv_3_2 = self.upconvolve(conv_3_2, base_filt*16, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2] \n",
    "                concat_2_1 = self.centre_crop_and_concat(conv_2_2, up_conv_3_2)\n",
    "                conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
    "                conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
    "                conv_2_4 = tf.layers.dropout(conv_2_4, rate = self.drop, training = self.training)\n",
    "                # Level one\n",
    "                up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
    "            \n",
    "            concat_1_1 = self.centre_crop_and_concat(conv_1_2, up_conv_2_1)\n",
    "            conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = tf.layers.dropout(conv_1_4, rate = self.drop, training = self.training)\n",
    "            # Level zero\n",
    "            up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [1,2,2])  # Stride previously [2,2,2]\n",
    "            concat_0_1 = self.centre_crop_and_concat(conv_0_2, up_conv_1_0)\n",
    "            conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = tf.layers.dropout(conv_0_4, rate = self.drop, training = self.training)\n",
    "            conv_out = tf.layers.conv3d(conv_0_4, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
    "            self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
    "            \n",
    "            # Note, this can be more easily visualised in a tool like tensorboard; Follows exact same format as in Paper.\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('Model Convolution output shape', conv_out.get_shape())\n",
    "                print('Model Argmax output shape', self.predictions.get_shape())\n",
    "            \n",
    "            do_weight = True\n",
    "            loss_weights = [0.00439314, 0.68209101, 0.31351585] # see section 1.4 # instead of [1, 150, 100, 1.0] \n",
    "            # Weighted cross entropy: approach adapts following code: https://stackoverflow.com/questions/44560549/unbalanced-data-and-weighted-cross-entropy\n",
    "            ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=conv_out, labels=labels_one_hot)\n",
    "            if do_weight:\n",
    "                weighted_loss = tf.reshape(tf.constant(loss_weights), [1, 1, 1, 1, num_classes]) # Format to the right size\n",
    "                weighted_one_hot = tf.reduce_sum(weighted_loss*labels_one_hot, axis = -1)\n",
    "                ce_loss = ce_loss * weighted_one_hot\n",
    "            self.loss = tf.reduce_mean(ce_loss) # Get loss\n",
    "            \n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            \n",
    "            self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Ensure correct ordering for batch-norm to work\n",
    "            with tf.control_dependencies(self.extra_update_ops):\n",
    "                self.train_op = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleUNet = False\n",
    "\n",
    "class UNetwork():\n",
    "    \n",
    "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
    "        # Produces the conv_batch_relu combination as in the paper\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "    \n",
    "        conv = tf.compat.v1.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
    "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
    "        conv = tf.compat.v1.layers.batch_normalization(conv, training = is_training)\n",
    "        conv = tf.nn.relu(conv) \n",
    "        return conv\n",
    "\n",
    "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
    "        # Upconvolution - two different implementations: the first is as suggested in the original Unet paper and the second is a more recent version\n",
    "        # Needs to be determined if these do the same thing\n",
    "        padding = 'valid'\n",
    "        if self.should_pad: padding = 'same'\n",
    "        # upsample_routine = tf.keras.layers.UpSampling3D(size = (scale,scale,scale)) # Uses tf.resize_images\n",
    "        # tensor = upsample_routine(tensor)\n",
    "        # conv = tf.layers.conv3d(tensor, filters, kernel, stride, padding = 'same',\n",
    "        #                                 kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
    "        # use_bias = False is a tensorflow bug\n",
    "        conv = tf.compat.v1.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias=False, \n",
    "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
    "        return conv\n",
    "\n",
    "    def centre_crop_and_concat(self, prev_conv, up_conv):\n",
    "        # If concatenating two different sized Tensors, centre crop the first Tensor to the right size and concat\n",
    "        # Needed if you don't have padding\n",
    "        p_c_s = prev_conv.get_shape()\n",
    "        u_c_s = up_conv.get_shape()\n",
    "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
    "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
    "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
    "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
    "        up_concat = tf.concat((prev_conv_crop, up_conv), 4)\n",
    "        return up_concat\n",
    "        \n",
    "    def __init__(self, base_filt = 8, in_depth = INPUT_DEPTH, out_depth = OUTPUT_DEPTH,\n",
    "                 in_size = INPUT_SIZE, out_size = OUTPUT_SIZE, num_classes = OUTPUT_CLASSES,\n",
    "                 learning_rate = 0.001, print_shapes = True, drop = 0.2, should_pad = False):\n",
    "        # Initialise your model with the parameters defined above\n",
    "        # Print-shape is a debug shape printer for convenience\n",
    "        # Should_pad controls whether the model has padding or not\n",
    "        # Base_filt controls the number of base conv filters the model has. Note deeper analysis paths have filters that are scaled by this value\n",
    "        # Drop specifies the proportion of dropped activations\n",
    "        \n",
    "        self.base_init = tf.compat.v1.truncated_normal_initializer(stddev=0.1) # Initialise weights\n",
    "        self.reg_init = tf.keras.regularizers.l2(l=0.5 * (0.1)) # Initialise regularisation (was useful)\n",
    "        \n",
    "        self.should_pad = should_pad # To pad or not to pad, that is the question\n",
    "        self.drop = drop # Set dropout rate\n",
    "        \n",
    "        with tf.compat.v1.variable_scope('3DuNet'):\n",
    "            self.training = tf.compat.v1.placeholder(tf.bool)\n",
    "            self.do_print = print_shapes\n",
    "            self.model_input = tf.compat.v1.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
    "            # Define placeholders for feed_dict\n",
    "            self.model_labels = tf.compat.v1.placeholder(tf.int32, shape = (None, out_depth, out_size, out_size, 1))\n",
    "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('Input features shape', self.model_input.get_shape())\n",
    "                print('Labels shape', labels_one_hot.get_shape())\n",
    "                \n",
    "            # Level zero\n",
    "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
    "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
    "            # Level one\n",
    "            max_1_1 = tf.compat.v1.layers.max_pooling3d(conv_0_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
    "            conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
    "            conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_2 = tf.compat.v1.layers.dropout(conv_1_2, rate = self.drop, training = self.training)\n",
    "            # Level two\n",
    "            max_2_1 = tf.compat.v1.layers.max_pooling3d(conv_1_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
    "            conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
    "            conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
    "            conv_2_2 = tf.compat.v1.layers.dropout(conv_2_2, rate = self.drop, training = self.training)\n",
    "            \n",
    "            if simpleUNet:\n",
    "                # Level one\n",
    "                up_conv_2_1 = self.upconvolve(conv_2_2, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
    "            else:\n",
    "                # Level three\n",
    "                max_3_1 = tf.compat.v1.layers.max_pooling3d(conv_2_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
    "                conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
    "                conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
    "                conv_3_2 = tf.compat.v1.layers.dropout(conv_3_2, rate = self.drop, training = self.training)\n",
    "                # Level two\n",
    "                up_conv_3_2 = self.upconvolve(conv_3_2, base_filt*16, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2] \n",
    "                concat_2_1 = self.centre_crop_and_concat(conv_2_2, up_conv_3_2)\n",
    "                conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
    "                conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
    "                conv_2_4 = tf.compat.v1.layers.dropout(conv_2_4, rate = self.drop, training = self.training)\n",
    "                # Level one\n",
    "                up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
    "            \n",
    "            concat_1_1 = self.centre_crop_and_concat(conv_1_2, up_conv_2_1)\n",
    "            conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
    "            conv_1_4 = tf.compat.v1.layers.dropout(conv_1_4, rate = self.drop, training = self.training)\n",
    "            # Level zero\n",
    "            up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [1,2,2])  # Stride previously [2,2,2]\n",
    "            concat_0_1 = self.centre_crop_and_concat(conv_0_2, up_conv_1_0)\n",
    "            conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
    "            conv_0_4 = tf.compat.v1.layers.dropout(conv_0_4, rate = self.drop, training = self.training)\n",
    "            conv_out = tf.compat.v1.layers.conv3d(conv_0_4, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
    "            self.predictions = tf.expand_dims(tf.argmax(input=conv_out, axis = -1), -1)\n",
    "            \n",
    "            # Note, this can be more easily visualised in a tool like tensorboard; Follows exact same format as in Paper.\n",
    "            \n",
    "            if self.do_print: \n",
    "                print('Model Convolution output shape', conv_out.get_shape())\n",
    "                print('Model Argmax output shape', self.predictions.get_shape())\n",
    "            \n",
    "            do_weight = True\n",
    "            loss_weights = [0.00439314, 0.68209101, 0.31351585] # see section 1.4 # instead of [1, 150, 100, 1.0] \n",
    "            # Weighted cross entropy: approach adapts following code: https://stackoverflow.com/questions/44560549/unbalanced-data-and-weighted-cross-entropy\n",
    "            ce_loss = tf.nn.softmax_cross_entropy_with_logits(logits=conv_out, labels=labels_one_hot)\n",
    "            if do_weight:\n",
    "                weighted_loss = tf.reshape(tf.constant(loss_weights), [1, 1, 1, 1, num_classes]) # Format to the right size\n",
    "                weighted_one_hot = tf.reduce_sum(input_tensor=weighted_loss*labels_one_hot, axis = -1)\n",
    "                ce_loss = ce_loss * weighted_one_hot\n",
    "            self.loss = tf.reduce_mean(input_tensor=ce_loss) # Get loss\n",
    "            \n",
    "            self.trainer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            \n",
    "            self.extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS) # Ensure correct ordering for batch-norm to work\n",
    "            with tf.control_dependencies(self.extra_update_ops):\n",
    "                self.train_op = self.trainer.minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the histogram equalised dataset if needed\n",
    "train = pickle.load(file = open('./pickles/heq_train.pkl', 'rb'))\n",
    "valid = pickle.load(file = open('./pickles/heq_valid.pkl', 'rb'))\n",
    "test = pickle.load(file = open('./pickles/heq_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Generate pre-processed augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training set: scaling & augmentation\n",
    "train_run = []\n",
    "augment_len = 10 # Number of perturbed data-points per raw datapoint\n",
    "for i in train:\n",
    "    (vox, lbl) = get_scaled_input(i)\n",
    "    train_run.append((vox, lbl))\n",
    "    for j in range(augment_len):\n",
    "        vox_a, lbl_a = get_random_perturbation(vox, lbl)\n",
    "        train_run.append((vox_a, lbl_a))\n",
    "        \n",
    "# Validation set: just scaling, no augmentation\n",
    "valid_run = []\n",
    "for i in valid:\n",
    "    (vox, lbl) = get_scaled_input(i)\n",
    "    valid_run.append((vox, lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Set model hyperparameters\n",
    "Set the model name `MODEL_NAME` and path `SAVE_PATH` to load a model, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001 # Model learning rate\n",
    "DROPOUT = 0.1\n",
    "BASE_FILT = 8    # Number of base filters\n",
    "BATCH_SIZE = 10  # Batch size - VRAM limited; originally 3\n",
    "PATIENCE = 5     # For early stopping: watching for validation loss increase\n",
    "NUM_EPOCHS = 100 # Maximum number of training epochs\n",
    "NUM_ITER = len(train_run) // BATCH_SIZE # Number of training steps per epoch\n",
    "MODEL_NAME = 'FIN10a1d8f' # Model name to LOAD FROM (looks IN SAVE_PATH directory)\n",
    "SAVE_PATH = \"./tf/\" \n",
    "LOGS_PATH = \"./tf_logs/\"\n",
    "LOAD_MODEL = True\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "if not os.path.exists(LOGS_PATH):\n",
    "    os.makedirs(LOGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Helper functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_raw(data, i, batch_size):\n",
    "    # Return separated x,y data from the i-th batch with the given batch size (batch_size)\n",
    "    return [x[0] for x in data[i:i+batch_size]], [y[1] for y in data[i:i+batch_size]]\n",
    "\n",
    "def get_pred_iou(predictions, lbl_original, ret_full = False, reswap = False):\n",
    "    # Get mean_iou for full batch\n",
    "    iou = []\n",
    "    for i in range(len(lbl_original)):\n",
    "        pred_cur = np.squeeze(predictions[i])\n",
    "        metric = get_mean_iou(pred_cur, lbl_original[i], ret_full = ret_full, reswap = reswap)\n",
    "        iou.append(metric)\n",
    "    if ret_full:\n",
    "        return np.mean(iou, axis = 0)\n",
    "    else:\n",
    "        return np.mean(iou) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Train model\n",
    "\n",
    "Train for `NUM_EPOCHS` epochs with early stopping on validation loss increase, with the patience `PATIENCE` epochs. Checkpoint the model periodically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(NUM_ITER, \" iterations per epoch\")\n",
    "tf.reset_default_graph()\n",
    "unet = UNetwork(drop = DROPOUT, base_filt = BASE_FILT, should_pad = True, learning_rate = LEARNING_RATE) # MODEL DEFINITION\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=4*PATIENCE)\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    writer = tf.summary.FileWriter(LOGS_PATH, graph=tf.get_default_graph())\n",
    "    if LOAD_MODEL:\n",
    "        print('Trying to load saved model...')\n",
    "        try:\n",
    "            print('Loading from: ', SAVE_PATH + MODEL_NAME+ '.meta')\n",
    "            restorer = tf.train.import_meta_graph(SAVE_PATH + MODEL_NAME+ '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(SAVE_PATH))\n",
    "            print(\"Model sucessfully restored\")\n",
    "        except IOError:\n",
    "            sess.run(init)\n",
    "            print(\"No previous model found, running default init\") \n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_IOUs = []\n",
    "    patience_cnt = 0\n",
    "    train_times = []\n",
    "    \n",
    "    for e in range(NUM_EPOCHS):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Shuffle the training data\n",
    "        random.shuffle(train_run)\n",
    "        \n",
    "        curr_train_loss = []\n",
    "        for i in range(NUM_ITER): # Iterate over batches within the epoch\n",
    "            print('Current epoch: ',e,', iteration: ',i,'/',NUM_ITER, end='\\r')\n",
    "            x, y = get_data_raw(train_run, i, BATCH_SIZE)\n",
    "            train_dict = {\n",
    "                unet.training: True,\n",
    "                unet.model_input: x,\n",
    "                unet.model_labels: y\n",
    "            }\n",
    "            _,loss = sess.run([unet.train_op, unet.loss], feed_dict = train_dict) # Train on batch and get train loss\n",
    "            curr_train_loss.append(loss)\n",
    "            \n",
    "        # Evaluate train and valid loss\n",
    "        train_loss = np.mean(curr_train_loss)\n",
    "        x, y = get_data_raw(valid_run, 0, len(valid_run)) # scaled\n",
    "        _, orig_y = get_data_raw(valid, 0, len(valid))    # non-scaled (for IOU evaluation)\n",
    "        valid_dict = {\n",
    "            unet.training: False,\n",
    "            unet.model_input: x,\n",
    "            unet.model_labels: y\n",
    "        }\n",
    "        val_loss = sess.run(unet.loss, feed_dict = valid_dict) # Get valid loss\n",
    "    \n",
    "        # Predict on validation set and calculate IOU\n",
    "        val_preds = np.squeeze(sess.run([unet.predictions], feed_dict = valid_dict))\n",
    "        iou = get_pred_iou(val_preds, orig_y, ret_full = True)\n",
    "        val_IOUs.append(iou)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Epoch \",e, \"\\t train_loss = \", train_loss, \"\\t val_loss = \", val_loss, \"\\t val_IOU = \", np.mean(iou))\n",
    "        \n",
    "        if e > 0:\n",
    "            start_time = train_times[-1]\n",
    "            if e % 5 == 0:\n",
    "                print('Saving model at epoch: ', e) # Save periodically\n",
    "                saver.save(sess, SAVE_PATH + MODEL_NAME, global_step = e)\n",
    "\n",
    "            if val_losses[-1] > val_losses[-2]:\n",
    "                patience_cnt += 1\n",
    "            else:\n",
    "                patience_cnt = 0\n",
    "                \n",
    "        train_times.append( time.time() - start_time )\n",
    "        \n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(\"Early stopping ...\")\n",
    "            saver.save(sess, SAVE_PATH + MODEL_NAME + '-final', global_step = e)\n",
    "            break\n",
    "            \n",
    "# The trace below is just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questo codice  stato tradotto \"automaticamente\" a tf2.0, non  stato testato\n",
    "print(NUM_ITER, \" iterations per epoch\")\n",
    "tf.compat.v1.reset_default_graph()\n",
    "unet = UNetwork(drop = DROPOUT, base_filt = BASE_FILT, should_pad = True, learning_rate = LEARNING_RATE) # MODEL DEFINITION\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables(), max_to_keep=4*PATIENCE)\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    writer = tf.compat.v1.summary.FileWriter(LOGS_PATH, graph=tf.compat.v1.get_default_graph())\n",
    "    if LOAD_MODEL:\n",
    "        print('Trying to load saved model...')\n",
    "        try:\n",
    "            print('Loading from: ', SAVE_PATH + MODEL_NAME+ '.meta')\n",
    "            restorer = tf.compat.v1.train.import_meta_graph(SAVE_PATH + MODEL_NAME+ '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(SAVE_PATH))\n",
    "            print(\"Model sucessfully restored\")\n",
    "        except IOError:\n",
    "            sess.run(init)\n",
    "            print(\"No previous model found, running default init\") \n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_IOUs = []\n",
    "    patience_cnt = 0\n",
    "    train_times = []\n",
    "    \n",
    "    for e in range(NUM_EPOCHS):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Shuffle the training data\n",
    "        random.shuffle(train_run)\n",
    "        \n",
    "        curr_train_loss = []\n",
    "        for i in range(NUM_ITER): # Iterate over batches within the epoch\n",
    "            print('Current epoch: ',e,', iteration: ',i,'/',NUM_ITER, end='\\r')\n",
    "            x, y = get_data_raw(train_run, i, BATCH_SIZE)\n",
    "            train_dict = {\n",
    "                unet.training: True,\n",
    "                unet.model_input: x,\n",
    "                unet.model_labels: y\n",
    "            }\n",
    "            _,loss = sess.run([unet.train_op, unet.loss], feed_dict = train_dict) # Train on batch and get train loss\n",
    "            curr_train_loss.append(loss)\n",
    "            \n",
    "        # Evaluate train and valid loss\n",
    "        train_loss = np.mean(curr_train_loss)\n",
    "        x, y = get_data_raw(valid_run, 0, len(valid_run)) # scaled\n",
    "        _, orig_y = get_data_raw(valid, 0, len(valid))    # non-scaled (for IOU evaluation)\n",
    "        valid_dict = {\n",
    "            unet.training: False,\n",
    "            unet.model_input: x,\n",
    "            unet.model_labels: y\n",
    "        }\n",
    "        val_loss = sess.run(unet.loss, feed_dict = valid_dict) # Get valid loss\n",
    "    \n",
    "        # Predict on validation set and calculate IOU\n",
    "        val_preds = np.squeeze(sess.run([unet.predictions], feed_dict = valid_dict))\n",
    "        iou = get_pred_iou(val_preds, orig_y, ret_full = True)\n",
    "        val_IOUs.append(iou)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Epoch \",e, \"\\t train_loss = \", train_loss, \"\\t val_loss = \", val_loss, \"\\t val_IOU = \", np.mean(iou))\n",
    "        \n",
    "        if e > 0:\n",
    "            start_time = train_times[-1]\n",
    "            if e % 5 == 0:\n",
    "                print('Saving model at epoch: ', e) # Save periodically\n",
    "                saver.save(sess, SAVE_PATH + MODEL_NAME, global_step = e)\n",
    "\n",
    "            if val_losses[-1] > val_losses[-2]:\n",
    "                patience_cnt += 1\n",
    "            else:\n",
    "                patience_cnt = 0\n",
    "                \n",
    "        train_times.append( time.time() - start_time )\n",
    "        \n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(\"Early stopping ...\")\n",
    "            saver.save(sess, SAVE_PATH + MODEL_NAME + '-final', global_step = e)\n",
    "            break\n",
    "            \n",
    "# The trace below is just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save training history\n",
    "np.savez('./' + MODEL_NAME + '.npz', train_losses=train_losses, val_losses=val_losses, val_IOUs=val_IOUs, \n",
    "         train_times=train_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "MODEL_NAME = 'm10a1d'\n",
    "hist = np.load('./' + MODEL_NAME + '.npz')\n",
    "train_losses = hist['train_losses']\n",
    "val_losses = hist['val_losses']\n",
    "val_IOUs = hist['val_IOUs']\n",
    "train_times = hist['train_times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show training history\n",
    "mean_val_IOUs = [np.mean(iou) for iou in val_IOUs]\n",
    "print(\"Minimum validation loss: \", np.min(val_losses), \" at epoch \", np.argmin(val_losses))\n",
    "print(\"Maximum validation IOU: \", np.max(mean_val_IOUs), \" at epoch \", np.argmax(mean_val_IOUs))\n",
    "print(\"Last validation loss: \", val_losses[-1])\n",
    "print(\"Last validation IOU: \", mean_val_IOUs[-1])\n",
    "\n",
    "# Loss Curves\n",
    "plt.figure()\n",
    "x = np.arange(len(train_losses))\n",
    "plt.plot(x, train_losses,'r-')\n",
    "plt.plot(x, val_losses,'b-')\n",
    "plt.legend(['Training loss', 'Validation loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0, 0.1)\n",
    "plt.show()\n",
    "\n",
    "# Validation IOU\n",
    "plt.figure()\n",
    "plt.plot(x, mean_val_IOUs,'b-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validatio IOU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model inference\n",
    "\n",
    "Predict with the trained model on the test set and calculate IOUs. \n",
    "\n",
    "#### Important: \n",
    "If sections 4.1 and 5 have not been executed, execute them before proceeding. <br>\n",
    "Iff section 6.4 has not been executed, run the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNetwork(drop = DROPOUT, base_filt = BASE_FILT, should_pad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST_MODEL_NAME = 'm10a1d-95'\n",
    "SAVE_PATH = \"./tf/\" \n",
    "BATCH_SIZE = 10 # was 3 originally\n",
    "\n",
    "# Load the test set\n",
    "test = pickle.load(file = open('./pickles/heq_test.pkl', 'rb'))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "test_predictions = []\n",
    "with tf.Session(config=config) as sess:\n",
    "    print('Loading saved model ...')\n",
    "    restorer = tf.train.import_meta_graph(SAVE_PATH + TEST_MODEL_NAME + '.meta')\n",
    "    restorer.restore(sess, SAVE_PATH + TEST_MODEL_NAME)\n",
    "    print(\"Model sucessfully restored\")\n",
    "    pred_out = [] # Predictions for each test scan\n",
    "    x_orig = []   # list of non-scaled scans\n",
    "    y_orig = []   # list of non-scaled annotations\n",
    "    x_in = []\n",
    "    y_in = []\n",
    "    i = 0\n",
    "    iou_out = []  # IOUs for each test scan\n",
    "\n",
    "    while i < len(test): # Iterate over batches\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for j in range(i, min(len(test), i + BATCH_SIZE)): # Iterate over samples within the batch\n",
    "            x_orig.append(np.copy(test[j][0]))\n",
    "            y_orig.append(np.copy(test[j][1]))\n",
    "            x_cur, y_cur = get_scaled_input(test[j])\n",
    "            x_batch.append(x_cur)\n",
    "            y_batch.append(y_cur)\n",
    "        if len(x_batch) == 0: break\n",
    "        print('Processing ', i)\n",
    "        x_in = x_in + x_batch\n",
    "        y_in = y_in + y_batch\n",
    "        test_dict = {\n",
    "            unet.training: False, # Whether to perform batch-norm at inference (Paper says this would be useful)\n",
    "            unet.model_input: x_batch,\n",
    "            unet.model_labels: y_batch\n",
    "        }\n",
    "        test_predictions = np.squeeze(sess.run([unet.predictions], feed_dict = test_dict))\n",
    "        if len(x_batch) == 1:\n",
    "            pred_out.append(test_predictions)\n",
    "        else:\n",
    "            pred_out.extend([np.squeeze(test_predictions[z, :, :, :]) for z in list(range(len(x_batch)))])\n",
    "        i += BATCH_SIZE\n",
    "\n",
    "    for i in range(len(y_orig)):\n",
    "        iou = get_mean_iou(pred_out[i], y_orig[i], ret_full = True)\n",
    "        print('Test scan', i,': IOUs: ', iou, 'Mean: ', np.mean(iou))\n",
    "        iou_out.append(np.mean(iou))\n",
    "    print('Mean test IOU', np.mean(iou_out), 'Std IOU', np.std(iou_out)) # mean over all test scans\n",
    "    \n",
    "# Save test predictions\n",
    "pickle.dump(file = open('./pickles/pred_' + TEST_MODEL_NAME + '.pkl', 'wb'), obj = pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODEL_NAME = 'm10a1d-95'\n",
    "SAVE_PATH = \"./tf/\" \n",
    "BATCH_SIZE = 10 # was 3 originally\n",
    "\n",
    "# Load the test set\n",
    "test = pickle.load(file = open('./pickles/heq_test.pkl', 'rb'))\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "test_predictions = []\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    print('Loading saved model ...')\n",
    "    restorer = tf.compat.v1.train.import_meta_graph(SAVE_PATH + TEST_MODEL_NAME + '.meta')\n",
    "    restorer.restore(sess, SAVE_PATH + TEST_MODEL_NAME)\n",
    "    print(\"Model sucessfully restored\")\n",
    "    pred_out = [] # Predictions for each test scan\n",
    "    x_orig = []   # list of non-scaled scans\n",
    "    y_orig = []   # list of non-scaled annotations\n",
    "    x_in = []\n",
    "    y_in = []\n",
    "    i = 0\n",
    "    iou_out = []  # IOUs for each test scan\n",
    "\n",
    "    while i < len(test): # Iterate over batches\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for j in range(i, min(len(test), i + BATCH_SIZE)): # Iterate over samples within the batch\n",
    "            x_orig.append(np.copy(test[j][0]))\n",
    "            y_orig.append(np.copy(test[j][1]))\n",
    "            x_cur, y_cur = get_scaled_input(test[j])\n",
    "            x_batch.append(x_cur)\n",
    "            y_batch.append(y_cur)\n",
    "        if len(x_batch) == 0: break\n",
    "        print('Processing ', i)\n",
    "        x_in = x_in + x_batch\n",
    "        y_in = y_in + y_batch\n",
    "        test_dict = {\n",
    "            unet.training: False, # Whether to perform batch-norm at inference (Paper says this would be useful)\n",
    "            unet.model_input: x_batch,\n",
    "            unet.model_labels: y_batch\n",
    "        }\n",
    "        test_predictions = np.squeeze(sess.run([unet.predictions], feed_dict = test_dict))\n",
    "        if len(x_batch) == 1:\n",
    "            pred_out.append(test_predictions)\n",
    "        else:\n",
    "            pred_out.extend([np.squeeze(test_predictions[z, :, :, :]) for z in list(range(len(x_batch)))])\n",
    "        i += BATCH_SIZE\n",
    "\n",
    "    for i in range(len(y_orig)):\n",
    "        iou = get_mean_iou(pred_out[i], y_orig[i], ret_full = True)\n",
    "        print('Test scan', i,': IOUs: ', iou, 'Mean: ', np.mean(iou))\n",
    "        iou_out.append(np.mean(iou))\n",
    "    print('Mean test IOU', np.mean(iou_out), 'Std IOU', np.std(iou_out)) # mean over all test scans\n",
    "    \n",
    "# Save test predictions\n",
    "pickle.dump(file = open('./pickles/pred_' + TEST_MODEL_NAME + '.pkl', 'wb'), obj = pred_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) If loading previously saved predictions\n",
    "\n",
    "TEST_MODEL_NAME = 'm10a1d-95'\n",
    "pred_out = pickle.load(file = open('./pickles/pred_' + TEST_MODEL_NAME + '.pkl', 'rb'))\n",
    "\n",
    "x_orig = []   # list of non-scaled scans\n",
    "y_orig = []\n",
    "for i in range(len(test)):\n",
    "    x_orig.append(np.copy(test[i][0]))\n",
    "    y_orig.append(np.copy(test[i][1]))\n",
    "    \n",
    "iou_out = []  # IOUs for each test scan\n",
    "for i in range(len(y_orig)):\n",
    "    iou = get_mean_iou(pred_out[i], y_orig[i], ret_full = True)\n",
    "    print('Test scan', i,': IOUs: ', iou, 'Mean: ', np.mean(iou))\n",
    "    iou_out.append(np.mean(iou))\n",
    "print('\\nMean test IOU', np.mean(iou_out), 'Std IOU', np.std(iou_out)) # mean over all test scans\n",
    "print(\"Best prediction on the test scan: \", np.argmax(iou_out))\n",
    "print(\"Worst prediction on the test scan: \", np.argmin(iou_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Visualise predictions\n",
    "\n",
    "To choose which test scan to show predictions on, pick `img_id` $\\in \\{0, 1, ... 9\\}$. \n",
    "The IOUs calculated in the above cell indicate which test scans obtained the best predictions and are therefore the best ones to visualise. \n",
    "\n",
    "*Note:* Run section 1.1 if it has not been executed yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Example: 'best' prediction\n",
    "\n",
    "We can see that even though the predictions for CG appear earlier and predictions for PZ disappear slightly later than they should, the predictions in the middle match the annotations very well. In general, the sample predictions indicate that the model rather overestimates the size of the prostate. \n",
    "\n",
    "**Press 'j' or 'k' to cycle through the plots in depth (Jupyter must be run locally!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 3\n",
    "x_cur = x_orig[img_id]\n",
    "y_cur = y_orig[img_id]\n",
    "y_upscale = upscale_segmentation(swap_axes(pred_out[img_id][:, :, :, np.newaxis]), np.shape(x_cur))\n",
    "multi_slice_viewer(x_cur, y_cur, y_upscale) # View  images, labels and predictions together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2. Example: 'worst' prediction\n",
    "\n",
    "We can see that there are almost no CG region predictions and the PZ is predicted at wrong positions. This may be caused by insufficient variability of the training data. \n",
    "\n",
    "**Press 'j' or 'k' to cycle through the plots in depth (Jupyter must be run locally!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 6\n",
    "x_cur = x_orig[img_id]\n",
    "y_cur = y_orig[img_id]\n",
    "y_upscale = upscale_segmentation(swap_axes(pred_out[img_id][:, :, :, np.newaxis]), np.shape(x_cur))\n",
    "multi_slice_viewer(x_cur, y_cur, y_upscale) # View  images, labels and predictions together "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "de580836a89f95ee31588502959a5d2f8747a15cf6743e969c0c25c962548b33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
